!wget https://raw.githubusercontent.com/mhulden/eztransformer/refs/heads/main/eztr.py
!mkdir downloads
!mv eztr.py downloads
## to clean up: rm downloads/eztr.py

%pip install -r "../requirements.txt"


from typing import Literal
from datasets import load_dataset

def load_data(
    lang: Literal['egyptian', 'gulf'],
    split: Literal['all', 'train', 'dev', 'no'] = 'all',
):
    lang = {'egyptian': 'arz', 'gulf': 'afb'}[lang]
    prefix = f"../data/{lang}/{lang}"
    files = {}
    if split in {"train", "all"}:
        files["train"] = f"{prefix}.trn"

    if split in {"train", "dev", "all", "no"}:
        files["valid"] = f"{prefix}.dev"

    if split in {"test", "all"}:
        files["test"] = f"{prefix}.tst"
    
    ds = load_dataset(
        'csv', delimiter="\t", data_files=files,
        column_names=["lemma", "features", "form"])

    if split == "no":
        return ds["valid"]
    
    return ds


def load_partial(train_size: int, valid_size: int|float, shift=0, lang='egyptian'):
    ''' load a subset of data '''

    data = load_data(lang=lang, split='train')
    
    # reduce data's size
    train = data['train'].select(range(shift, shift+train_size))
    
    if isinstance(valid_size, float):
        valid_size = int(valid_size * train_size)
    # returning training data instead of valid, to avoid peeking into dev data while debugging
    valid = data['train'].select(range(shift+train_size, shift+train_size+valid_size))
    
    data['train'] = train
    data['valid'] = valid
    
    return data


batch_size = 512
num_proc = 4
train_size = 2048 * 2
valid_size = 256 

data = load_partial(train_size=train_size, valid_size=valid_size, shift=3491)

# turn to characters
data = data.map(
    lambda batch: {
        "lemma": [list(s) for s in batch["lemma"]],
        "form": [list(t) for t in batch["form"]],
        "features": [f.split(';') for f in batch["features"]],
    },
    batched=True,
    batch_size=batch_size,
    num_proc=num_proc,
)

data = data.map(
    lambda batch: {
        'src': [l+f for l, f in zip(batch["lemma"], batch["features"])]
    },
    batched=True,
    batch_size=batch_size,
    num_proc=num_proc,
    remove_columns=['features', 'lemma']
)

# only for compatibility with eztransformer's build_vocab
data = data.map(
    lambda batch: {
        'src': [' '.join(item) for item in batch['src']],
        'form': [' '.join(item) for item in batch['form']],
    },
    batched=True,
    batch_size=batch_size,
    num_proc=num_proc,
)


from downloads.eztr import EZTransformer

apple_device = True

# Initialize model
trf = EZTransformer(
    device = 'mps' if apple_device else 'cpu', # Change device as needed; 'cuda' (NVIDIA), 'mps' (Apple), or 'cpu'
    # learning rate
    lrt = 1e-3,
    # batch size
    bts = 256,
    # embedding
    eed = 256,
    ded = 256,
    # hidden size:
    ehs = 512,
    dhs = 512,
    # layers:
    enl = 2,
    dnl = 2,
    # heads:
    eah = 4,
    dah = 4,

    save_best = 10,
    # dropout
    drp=0.1,

    # lst
    # cnm
    )

def ez_train(train_data, valid_data, print_validation_examples):
    if not isinstance(train_data, list):
        train_data = list(zip(train_data["src"], train_data["form"]))
        valid_data = list(zip(valid_data["src"], valid_data["form"]))

    # Train model
    trf.fit(
        train_data = train_data, 
        valid_data = valid_data, 
        print_validation_examples = 0, 
        max_epochs = 40,
        warmup=(train_size//batch_size)*20,
        )

    trf.print_validation_examples(valid_data, print_validation_examples)

ez_train(data['train'], data['valid'], 10)









