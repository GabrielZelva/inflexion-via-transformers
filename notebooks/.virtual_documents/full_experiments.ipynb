





!wget https://raw.githubusercontent.com/mhulden/eztransformer/refs/heads/main/eztr.py
!mkdir downloads
!mv eztr.py downloads
## to clean up: rm downloads/eztr.py

#%pip install -r "../requirements.txt"
IS_COLAB = False
IS_MAC = True

device = "cuda" if IS_COLAB else "mps" if IS_MAC else "cpu"


import matplotlib.pyplot as plt
from typing import Literal
from datasets import load_dataset
from downloads.eztr import EZTransformer





# Load the default data ########################################################

def load_data(
    lang: Literal['egyptian', 'gulf'],
    split: Literal['all', 'train', 'dev', 'no'] = 'all',
):
    lang = {'egyptian': 'arz', 'gulf': 'afb'}[lang]
    
    prefix = f"data/{lang}/{lang}"
    if IS_COLAB:
       prefix = '/content/' + prefix
    else:
       prefix = '../' + prefix
       
    files = {}
    if split in {"train", "all"}:
        files["train"] = f"{prefix}.trn"

    if split in {"train", "dev", "all", "no"}:
        files["dev"] = f"{prefix}.dev"

    if split in {"test", "all"}:
        files["test"] = f"{prefix}.tst"

    ds = load_dataset(
        'csv', delimiter="\t", data_files=files,
        column_names=["lemma", "features", "form"])

    if split == "no":
        return ds["dev"]

    return ds

# Aux function #################################################################

def process_split(split_data, batch_size: int = 100, num_proc: int = 4):
    # turn to characters
    split_data = split_data.map(
        lambda batch: {
            "lemma": [list(s) for s in batch["lemma"]],
            "form": [list(t) for t in batch["form"]],
            "features": [f.split(';') for f in batch["features"]],
        },
        batched=True,
        batch_size=batch_size,
        num_proc=num_proc,
    )

    split_data = split_data.map(
        lambda batch: {
            'src': [l+f for l, f in zip(batch["lemma"], batch["features"])]
        },
        batched=True,
        batch_size=batch_size,
        num_proc=num_proc,
        remove_columns=['features', 'lemma']
    )

    # only for compatibility with eztransformer's build_vocab
    split_data = split_data.map(
        lambda batch: {
            'src': [' '.join(item) for item in batch['src']],
            'form': [' '.join(item) for item in batch['form']],
        },
        batched=True,
        batch_size=batch_size,
        num_proc=num_proc,
    )
    return split_data

# Custom data loading ##########################################################

def load_partial(train_size: int,
                 dev_size: int,
                 test_size: int, # This test split is NOT being used downstream for now, but I am keeping it here for consistency
                 batch_size: int,
                 num_proc: int = 4,
                 lang='egyptian'):

    full_data_splits = load_data(lang=lang, split='all')

    processed_data = {}

    processed_data['train'] = full_data_splits['train'].select(range(0, train_size))

    processed_data['dev'] = full_data_splits['dev'].select(range(0, dev_size))

    processed_data['test'] = full_data_splits['test'].select(range(0, test_size))

    for split_name, split_dataset in processed_data.items():
      processed_data[split_name] = process_split(split_dataset, batch_size, num_proc)

    return processed_data

# Get a new model ##############################################################

def get_new_model():
  return EZTransformer(
    device = device,
    # learning rate
    lrt = 1e-3,
    # batch size
    bts = 256,
    # embedding
    eed = 256,
    ded = 256,
    # hidden size:
    ehs = 512,
    dhs = 512,
    # layers:
    enl = 2,
    dnl = 2,
    # heads:
    eah = 4,
    dah = 4,

    save_best = 10,

    # dropout
    drp=0.1
    )

# Model training ###############################################################

def ez_train(model, train_data, valid_data, epochs = 100, print_validation_examples = 0):
    if not isinstance(train_data, list):
        train_data = list(zip(train_data["src"], train_data["form"]))
        valid_data = list(zip(valid_data["src"], valid_data["form"]))

    # Train model
    history = model.fit(
        train_data = train_data,
        valid_data = valid_data,
        print_validation_examples = 0,
        max_epochs = epochs,
        return_history=True
        )

    model.print_validation_examples(valid_data, print_validation_examples)

    return history

# History analysis #############################################################

def training_history(history):

  min_epoch = history.loc[history["val_loss"].idxmin(), "epoch"]
  text = f"The lowest dev loss was reached in epoch {min_epoch}."

  plt.figure()
  plt.plot(history["epoch"], history["train_loss"], label="Train loss")
  plt.plot(history["epoch"], history["val_loss"], label="Dev loss")

  plt.xlabel("Epoch")
  plt.ylabel("Loss")
  plt.title("Training and Dev loss")
  plt.legend()

  plt.show()
  print(text)


# Model testing ################################################################
def test_model(model, language):
  # Preprocess the data
  raw_test_data = load_data(lang=language, split="test")
  processed_test_data = process_split(raw_test_data["test"])

  test_inputs = processed_test_data["src"]
  test_targets = processed_test_data["form"]

  test_examples_for_printing = list(zip(processed_test_data['src'], processed_test_data['form']))

  # Use the default EZTF evaluation method
  model.score(test_inputs, test_targets)

  # Show examples
  model.print_validation_examples(test_examples_for_printing, 3)











data = load_partial(train_size=1000,
                    dev_size = 100,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "egyptian")





%%capture captured_output

egyptian1k = get_new_model()

history = ez_train(egyptian1k,
                   data['train'],
                   data['dev'],
                   epochs = 100)





training_history(history)





test_model(egyptian1k, "egyptian")





test_model(egyptian1k, "gulf")





%%capture captured_output

data = load_partial(train_size=5000,
                    dev_size = 500,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "egyptian")

egyptian5k = get_new_model()

history = ez_train(egyptian5k,
                   data['train'],
                   data['dev'],
                   epochs = 100)





training_history(history)





test_model(egyptian5k, "egyptian")





test_model(egyptian5k, "gulf")





%%capture captured_output

data = load_partial(train_size=10000,
                    dev_size = 1000,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "egyptian")

egyptian10k = get_new_model()

history = ez_train(egyptian10k,
                   data['train'],
                   data['dev'],
                   epochs = 20)





training_history(history)





test_model(egyptian10k, "egyptian")





test_model(egyptian10k, "gulf")








%%capture captured_output

data = load_partial(train_size=1000,
                    dev_size = 100,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "gulf")

gulf1k = get_new_model()

history = ez_train(gulf1k,
                   data['train'],
                   data['dev'],
                   epochs = 50)


training_history(history)





test_model(gulf1k, "gulf")


test_model(gulf1k, "egyptian")





%%capture captured_output

data = load_partial(train_size=5000,
                    dev_size = 500,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "gulf")

gulf5k = get_new_model()

history = ez_train(gulf5k,
                   data['train'],
                   data['dev'],
                   epochs = 50)


training_history(history)





test_model(gulf5k, "gulf")


test_model(gulf5k, "egyptian")





%%capture captured_output

data = load_partial(train_size=10000,
                    dev_size = 1000,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "gulf")

gulf10k = get_new_model()

history = ez_train(gulf10k,
                   data['train'],
                   data['dev'],
                   epochs = 20)


training_history(history)





test_model(gulf10k, "gulf")


test_model(gulf10k, "egyptian")








# used for creating deep copies of models before further fine-tuning
import copy








%%capture captured_output

data = load_partial(train_size=1000,
                    dev_size = 100,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "gulf")

egyptian10kgulf1k = copy.deepcopy(egyptian10k)

history = ez_train(egyptian10kgulf1k,
                   data['train'],
                   data['dev'],
                   epochs = 50)


training_history(history)





test_model(egyptian10kgulf1k, "gulf")


test_model(egyptian10kgulf1k, "egyptian")





%%capture captured_output

data = load_partial(train_size=5000,
                    dev_size = 500,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "gulf")

egyptian10kgulf5k = copy.deepcopy(egyptian10k)

history = ez_train(egyptian10kgulf5k,
                   data['train'],
                   data['dev'],
                   epochs = 50)





training_history(history)





test_model(egyptian10kgulf5k, "gulf")


test_model(egyptian10kgulf5k, "egyptian")





%%capture captured_output

data = load_partial(train_size=10000,
                    dev_size = 1000,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "gulf")

egyptian10kgulf10k = copy.deepcopy(egyptian10k)

history = ez_train(egyptian10kgulf10k,
                   data['train'],
                   data['dev'],
                   epochs = 20)





training_history(history)





test_model(egyptian10kgulf10k, "gulf")


test_model(egyptian10kgulf10k, "egyptian")








%%capture captured_output

data = load_partial(train_size=1000,
                    dev_size = 100,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "egyptian")

gulf10kegyptian1k = copy.deepcopy(gulf10k)

history = ez_train(gulf10kegyptian1k,
                   data['train'],
                   data['dev'],
                   epochs = 50)


training_history(history)





test_model(gulf10kegyptian1k, "egyptian")


test_model(gulf10kegyptian1k, "gulf")





%%capture captured_output

data = load_partial(train_size=5000,
                    dev_size = 500,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "egyptian")

gulf10kegyptian5k = copy.deepcopy(gulf10k)

history = ez_train(gulf10kegyptian5k,
                   data['train'],
                   data['dev'],
                   epochs = 50)


training_history(history)





test_model(gulf10kegyptian5k, "egyptian")


test_model(gulf10kegyptian5k, "gulf")





%%capture captured_output

data = load_partial(train_size=10000,
                    dev_size = 1000,
                    test_size = 0,
                    batch_size = 100,
                    num_proc= 4,
                    lang = "egyptian")

gulf10kegyptian10k = copy.deepcopy(gulf10k)

history = ez_train(gulf10kegyptian10k,
                   data['train'],
                   data['dev'],
                   epochs = 20)


training_history(history)





test_model(gulf10kegyptian10k, "egyptian")


test_model(gulf10kegyptian10k, "gulf")









